{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 6818613,
          "sourceType": "datasetVersion",
          "datasetId": 3921884
        }
      ],
      "dockerImageVersionId": 30558,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Object Detection with U-Net and YOLO-inspired Architecture\n\nThis notebook implements an object detection model combining U-Net for feature extraction and a YOLO-inspired classification head. The model is trained on a custom dataset with bounding box annotations.\n\n## Imports"
    },
    {
      "cell_type": "code",
      "source": "import os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom PIL import ImageDraw as D\nimport cv2\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Dataset Preparation\n\nLoad and preprocess the dataset, including images and bounding box annotations from a CSV file."
    },
    {
      "cell_type": "code",
      "source": "def find_csv_file(directory):\n    \"\"\"Find the first CSV file in the specified directory.\"\"\"\n    try:\n        for f in os.listdir(directory):\n            if f.endswith(\".csv\"):\n                return f\n        raise FileNotFoundError(\"No CSV file found in the directory.\")\n    except Exception as e:\n        print(f\"Error accessing directory {directory}: {e}\")\n        raise\n\ndataset_path = \"/kaggle/input/project-dataset\"\ntry:\n    csv_file = find_csv_file(dataset_path)\n    df = pd.read_csv(os.path.join(dataset_path, csv_file))\n    df.drop(df.columns[0], axis=1, inplace=True)  # Drop unnamed index column\n    df = df.sort_values(by=\"file_name\", axis=0)\n    print(\"Dataset loaded successfully:\")\n    print(df.head())\nexcept Exception as e:\n    print(f\"Error loading dataset: {e}\")\n    raise",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Model Architecture\n\nDefine the neural network models: `Net` (U-Net based feature extractor) and `Classify` (YOLO-inspired classification head)."
    },
    {
      "cell_type": "code",
      "source": "class Net(nn.Module):\n    \"\"\"U-Net inspired feature extractor for object detection.\"\"\"\n    def __init__(self, width=224, height=224, in_channels=3):\n        super(Net, self).__init__()\n        # Downsampling Network\n        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=128, kernel_size=7, bias=False)\n        self.mx1 = nn.MaxPool2d(kernel_size=3, stride=2)\n        self.bn1 = nn.BatchNorm2d(num_features=128)\n        \n        self.conv2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, bias=False)\n        self.mx2 = nn.MaxPool2d(kernel_size=3)\n        self.bn2 = nn.BatchNorm2d(num_features=256)\n        \n        self.conv3 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, bias=False)\n        self.mx3 = nn.MaxPool2d(kernel_size=3)\n        self.bn3 = nn.BatchNorm2d(num_features=512)\n        \n        self.conv4 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3)\n        self.mx4 = nn.MaxPool2d(kernel_size=3)\n        \n        # Upsampling Network\n        self.prep_dconv = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=5, stride=3)\n        self.dconv1 = nn.ConvTranspose2d(in_channels=1024, out_channels=256, kernel_size=5, stride=3)\n        self.dconv2 = nn.ConvTranspose2d(in_channels=512, out_channels=128, kernel_size=5, stride=3, output_padding=1)\n        self.dconv3 = nn.ConvTranspose2d(in_channels=256, out_channels=3, kernel_size=9, stride=2, output_padding=1)\n\n    def forward(self, x):\n        # Downsampling path\n        x1 = self.conv1(x)\n        x2 = self.mx1(x1)\n        x3 = self.bn1(x2)\n        x4 = F.relu(x3)\n        \n        x5 = self.conv2(x4)\n        x6 = self.mx2(x5)\n        x7 = self.bn2(x6)\n        x8 = F.relu(x7)\n        \n        x9 = self.conv3(x8)\n        x10 = self.mx3(x9)\n        x11 = self.bn3(x10)\n        x12 = F.relu(x11)\n        \n        x13 = self.conv4(x12)\n        x14 = self.mx4(x13)\n        x15 = F.relu(x14)\n        \n        # Upsampling path with skip connections\n        x16 = self.prep_dconv(x15)\n        x17 = F.relu(x16)\n        x18 = torch.cat((x12, x17), 1)\n        x19 = F.relu(self.dconv1(x18))\n        x20 = torch.cat((x8, x19), 1)\n        x21 = F.relu(self.dconv2(x20))\n        x22 = torch.cat((x4, x21), 1)\n        x23 = F.relu(self.dconv3(x22))\n        return x23",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "class Classify(nn.Module):\n    \"\"\"YOLO-inspired classification head for object detection.\"\"\"\n    def __init__(self):\n        super(Classify, self).__init__()\n        self.c_conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=7, stride=2)\n        self.x_mx1 = nn.AvgPool2d(kernel_size=3, stride=3)\n        self.c_conv2 = nn.Conv2d(in_channels=16, out_channels=49, kernel_size=5, stride=2)\n        self.x_mx2 = nn.AvgPool2d(kernel_size=3, stride=3)\n\n    def forward(self, x):\n        x = self.c_conv1(x)\n        x = F.relu(self.x_mx1(x))\n        x = self.c_conv2(x)\n        x = self.x_mx2(x)\n        return x",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Custom Dataset\n\nDefine a custom PyTorch dataset to load images and their corresponding bounding box annotations."
    },
    {
      "cell_type": "code",
      "source": "class CustDat(torch.utils.data.Dataset):\n    \"\"\"Custom dataset for loading images and bounding box annotations.\"\"\"\n    def __init__(self, folder_path=\"/kaggle/input/project-dataset\"):\n        self.folder_path = folder_path\n        try:\n            self.all_images = sorted(self._get_all_images(self.folder_path))\n            self.len_images = len(self.all_images)\n            csv_file = find_csv_file(self.folder_path)\n            self.df = pd.read_csv(os.path.join(self.folder_path, csv_file))\n            self.df.drop([self.df.columns[0]], axis=1, inplace=True)\n            self.df = self.df.sort_values(by=\"file_name\", axis=0)\n            self.names = self.df.object.unique()\n            self.mp = dict(zip(self.names, range(len(self.names))))\n            self.transform = transforms.ToTensor()\n        except Exception as e:\n            print(f\"Error initializing dataset: {e}\")\n            raise\n\n    def _get_all_images(self, folder_path):\n        \"\"\"Retrieve all .jpg images from the folder.\"\"\"\n        return [f for f in os.listdir(folder_path) if f.endswith(\".jpg\")]\n\n    def __len__(self):\n        return self.len_images\n\n    def __getitem__(self, idx):\n        try:\n            img_name = self.all_images[idx]\n            img = Image.open(os.path.join(self.folder_path, img_name))\n            _, xmin, xmax, ymin, ymax, cl = self.df.iloc[idx]\n            img = self.transform(img)\n            return img, torch.Tensor([self.mp[cl], xmin, xmax, ymin, ymax])\n        except Exception as e:\n            print(f\"Error loading item {idx}: {e}\")\n            raise",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Data Loader\n\nSet up the data loader for batch processing during training."
    },
    {
      "cell_type": "code",
      "source": "batch_size = 16\ntry:\n    train_dl = torch.utils.data.DataLoader(\n        CustDat(),\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=2,\n        drop_last=True\n    )\n    print(f\"DataLoader created with batch size {batch_size}\")\nexcept Exception as e:\n    print(f\"Error creating DataLoader: {e}\")\n    raise",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Model Initialization\n\nInitialize the models and move them to the appropriate device."
    },
    {
      "cell_type": "code",
      "source": "model = Net().to(device)\nclas = Classify().to(device)\nprint(\"Models initialized and moved to\", device)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Training Setup\n\nDefine hyperparameters, optimizers, and loss functions."
    },
    {
      "cell_type": "code",
      "source": "num_epochs = 60\nlr = 0.001\nnum_classes = 20\nS = 7  # Grid size for YOLO-inspired output\n\noptimizer_model = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0001)\noptimizer_clas = torch.optim.Adam(clas.parameters(), lr=lr, weight_decay=0.0001)\nclassification_loss_fn = nn.CrossEntropyLoss()\n\ntrain_loss = []  # Track loss per epoch",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Training Loop\n\nTrain the model over multiple epochs, computing classification, localization, and objectness losses."
    },
    {
      "cell_type": "code",
      "source": "for epoch in range(num_epochs):\n    epoch_loss = 0\n    batch_count = 0\n    for images, igs in train_dl:\n        batch_count += 1\n        images = images.to(device)\n        igs = igs.to(device)\n        \n        # Forward pass\n        model_out = model(images)\n        class_output = clas(model_out)\n        class_output = class_output.flatten(-2)\n        \n        # Compute grid cell indices\n        x = igs[:, 1:3].mean(dim=1)\n        y = igs[:, 3:].mean(dim=1)\n        width = (igs[:, 2] - igs[:, 1])\n        height = (igs[:, 4] - igs[:, 3])\n        row = (y / (224 / S)).int()\n        col = (x / (224 / S)).int()\n        num = row * S + col\n        \n        # Normalize coordinates\n        rem_x = x % (224 / S)\n        rem_y = y % (224 / S)\n        normalised_x = rem_x / (224 / S)\n        normalised_y = rem_y / (224 / S)\n        normalised_w = width / 224\n        normalised_h = height / 224\n        root_norm_w = torch.sqrt(normalised_w)\n        root_norm_h = torch.sqrt(normalised_h)\n        \n        # Compute losses\n        out = class_output[torch.arange(batch_size), num]\n        class_loss = classification_loss_fn(out[..., :20], igs[:, 0].long())\n        \n        out_xywh = F.sigmoid(out[:, 21:])\n        xy_loss = (out_xywh[:, 0] - normalised_x) ** 2 + (out_xywh[:, 1] - normalised_y) ** 2\n        wh_loss = (torch.sqrt(out_xywh[:, 2]) - root_norm_w) ** 2 + (torch.sqrt(out_xywh[:, 3]) - root_norm_h) ** 2\n        localization_loss = xy_loss + wh_loss\n        \n        obj_prob = F.sigmoid(class_output[..., 20])\n        temp = torch.zeros_like(obj_prob)\n        temp[torch.arange(batch_size), num] = 1\n        obj_loss = (temp - obj_prob) ** 2\n        \n        # Total loss\n        loss = class_loss + localization_loss.sum() + obj_loss.sum()\n        epoch_loss += loss.cpu().detach().numpy()\n        \n        # Backward pass\n        optimizer_model.zero_grad()\n        optimizer_clas.zero_grad()\n        loss.backward()\n        optimizer_model.step()\n        optimizer_clas.step()\n    \n    train_loss.append(epoch_loss / batch_count)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / batch_count:.4f}\")\n\n# Save the trained models\ntorch.save(model.state_dict(), \"unet_model.pth\")\ntorch.save(clas.state_dict(), \"classify_model.pth\")\nprint(\"Models saved as unet_model.pth and classify_model.pth\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Visualization and Evaluation\n\nVisualize the model's predictions on a test image."
    },
    {
      "cell_type": "code",
      "source": "# Get a test image\ntry:\n    test_batch = next(iter(train_dl))\n    test_img = transforms.ToPILImage()(test_batch[0][5].cpu().detach())\n    test_img.save(\"test_img.jpg\")\nexcept Exception as e:\n    print(f\"Error loading test image: {e}\")\n    raise\n\n# Predict bounding box\nwith torch.no_grad():\n    img = test_batch[0][5][None, ...].to(device)\n    cl = clas(model(img)).flatten(-2).squeeze()\n    mx_ind = cl[:, 20].argmax()\n    class_idx = cl[mx_ind][:20].argmax()\n    obj_conf = F.sigmoid(cl[mx_ind][20])\n    bbox = F.sigmoid(cl[mx_ind][21:]) * torch.tensor([32, 32, 224, 224], device=device)\n    xmin, ymin, width, height = bbox.cpu().numpy().astype(int)\n    rw = int(mx_ind / 7) * 32 + xmin\n    col = int(mx_ind % 7) * 32 + ymin\n    print(f\"Predicted Class: {list(df.object.unique())[class_idx]}, Confidence: {obj_conf:.4f}\")\n    print(f\"Bounding Box: (x={rw}, y={col}, w={width}, h={height})\")\n\n# Draw bounding box\niggg = cv2.imread(\"test_img.jpg\")\nleft = (rw, col)\nright = (rw + width, col + height)\nig_rec = cv2.rectangle(iggg, left, right, (0, 255, 0), 2)\nplt.imshow(cv2.cvtColor(ig_rec, cv2.COLOR_BGR2RGB))\nplt.title(f\"Predicted Class: {list(df.object.unique())[class_idx]}\")\nplt.axis(\"off\")\nplt.show()\n\n# Plot training loss\nplt.figure()\nplt.plot(train_loss)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss vs Epoch\")\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}